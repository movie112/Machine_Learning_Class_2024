# Assignment 09

# Logistric regression based on non-linear feature functions

## 1. Problem definition

### (1) Training Data

The training data consists of $\{(x_i, y_i, \ell_i)\}_{i=1}^n\}$ where $(x_i, y_i) \in \mathbb{R}^2$ represent a point in 2-dimensional coordinate and $\ell_i \in \{0, 1\}$ represents its class label.

### (2) Non-linear feature function

For a given data point $(x, y)$, a set of feature functions $\phi(x, y) = \{ \phi_k(x, y) \}_{k=1}^K$ is employed to characterize data in the feature space in such a way that the data is better separable:

$$
\phi(x, y) = (\phi_1(x, y), \phi_2(x, y), \cdots, \phi_K(x, y))
$$

### (3) Linear regression function

Let $p = (1, x, y)$ be a data point and $\theta = (\theta_1, \theta_2, \cdots, \theta_K)$ in $\mathbb{R}^K$ be a set of model parameters. The linear regression function is defined based on a set of feature functions $\phi(x, y) = (\phi_1(x, y), \phi_2(x, y), \cdots, \phi_K(x, y))$ in $\mathbb{R}^K$ as defined by:

$$
f(\theta; \phi, x, y) = \theta_1 \cdot \phi_1(x, y) + \theta_2 \cdot \phi_2(x, y) + \cdots + \theta_K \cdot \phi_K(x, y) = \theta^{T} \phi(x, y)
$$

where $\theta \in \mathbb{R}^K$ and $\phi(x, y) \in \mathbb{R}^K$.

### (4) Activation by Sigmoid function

The sigmoid function is defined by:

$$
\sigma(z) = \frac{1}{1 + \exp{(-z)}}
$$

The derivative of sigmoid function is defined by:

$$
\sigma'(z) = \sigma(z) (1 - \sigma(z)).
$$

### (5) Logistic regression function

The logistic regression function is defined by:

$$
h(\theta; \phi, x, y) = \sigma(f(\theta; \phi, x, y)).
$$

### (6) Objective function

The objective function for the binary classification based on the logistic regression function is defined by:

$$
\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \{ - \ell_i \log{(h_i)} - (1 - \ell_i) \log{(1 - h_i)} \}
$$

where $h_i = \sigma(f(\theta; \phi, x_i, y_i))$.

### (7) Optimization using the gradient descent algorithm

The optimization of the objective function $\mathcal{L}(\theta)$ with respect to the model parameters $\theta$ using the gradient descent algorithm is given by:

$$
\theta^{t+1} \coloneqq \theta^{t} - \eta \nabla \mathcal{L}(\theta^t)  
$$

where $t$ denotes iteration and $\eta$ denotes learning rate.

### (8) Optimal classifier

The classifier for point $(x, y)$ is given by the logistic regression function with obtained model parameters $\theta^*$ as given by:

$$
h(\theta^*; \phi, x, y)
$$

where

$$
\theta^* = \arg\min_\theta \mathcal{L}(\theta)
$$

## 2. code completion

- complete `util.py` code
- run all the code cells in the jupyter notebook
- make `git commit` for the `util.py` file in such a way that your working progress is effectively presented

## 3. submission

1. jupyter notebook source files [.ipynb] - complete the jupyter notebook with all the results
   - `09.ipynb`
2. jupyter notebook pdf files [.pdf] - export the complete jupyter notebook to HTML and then PDF
   - `09.pdf`
3. GitHub history pdf file [.pdf] - export the GitHub history of the utility python file to a pdf file
   - `09_history.pdf`
4. python files [.py] - complete the utility python file
   - `util.py`
