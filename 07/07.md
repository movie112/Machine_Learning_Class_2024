# Assignment 07

# Linear Regression

## 1. Problem Definition

### (1) Formulation of the linear regression

- training dataset is given by a set of three dimensional points

$$
\{ (x_i, y_i, z_i) \}_{i=1}^n = \{ (x_1, y_1, z_1), (x_2, y_2, z_2), \cdots, (x_n, y_n, z_n) \}, \quad (x_i, y_i, z_i) \in \mathbb{R}^3, \forall i
$$

- the model function $f$ predicts $\hat{z} = f(\theta; x, y)$ for a given point $(x, y) \in \mathbb{R}^2$ where the model is associated with a set of model parameters $\theta = (\theta_0, \theta_1, \theta_2) \in \mathbb{R}^3$

$$
f(\theta ; x, y) = \theta_0 + \theta_1 x + \theta_2 y, \quad (\theta_0, \theta_1, \theta_2) \in \mathbb{R}^3
$$

- the prediction function $f(\theta)$ is determined by model parameters

$$
\theta = (\theta_0, \theta_1, \theta_2) \in \mathbb{R}^3
$$

- residual $\gamma_i(\theta)$ associated with a point $(x_i, y_i, z_i)$ is determined by  

$$
\gamma_{i}(\theta) = \hat{f}(\theta ; x_i, y_i) - z_i
$$

- objective function is defined based on the average residual over the given data

$$
\mathcal{L}(\theta) = \frac{1}{2 n} \sum_{i=1}^n \gamma_{i}^2(\theta) = \frac{1}{2 n} \sum_{i=1}^n (\hat{f}(\theta ; x_i, y_i) - z_i)^2 = \frac{1}{2 n} \sum_{i=1}^n (\theta_0 + \theta_1 x_i + \theta_2 y_i - z_i)^2 
$$

- objective function can be written in the matrix representation as defined by:

$$
\begin{align*}
\mathcal{L}(\theta) &= \frac{1}{2 n} \| A \theta - z \|_2^2\\
&= \frac{1}{2 n} (\theta^T A^T - z^T) (A \theta - z)\\
&= \frac{1}{2 n} (\theta^T A^T A \theta - \theta^T A^T z - z^T A \theta + z^T z)
\end{align*}
$$

$$
A =
\begin{bmatrix}
1 & x_{1} & y_{1}\\
1 & x_{2} & y_{2}\\
\vdots & \vdots & \vdots \\
1 & x_{n} & y_{n}
\end{bmatrix},
\quad
\theta =
\begin{bmatrix}
\theta_{0} \\
\theta_{1} \\
\theta_{2}
\end{bmatrix},
\quad
z =
\begin{bmatrix}
z_{1} \\
z_{2} \\
\vdots \\
z_{n}
\end{bmatrix}
$$

- solution

$$
\theta^* = \arg\min_{\theta} \mathcal{L}(\theta)
$$

### (2) Optimization by Gradient Descent

- iterative optimization with gradient descent for the model parameters

$$
\theta^{(t+1)} \coloneqq \theta^{(t)} - \eta \, \nabla_\theta \mathcal{L}(\theta^{(t)})
$$

where $\eta > 0$ in $\mathbb{R}$ is called learning rate.

- the gradient descent step is taken for the model parameter vector $\theta = (\theta_0, \theta_1, \theta_2)$
- the gradient descent step begins from the initial condition $\theta^{(0)} = (\theta_0^{(0)}, \theta_1^{(0)}, \theta_2^{(0)})$

### (3) Regression Surface

- three dimensional surface of regression function $f$ with optimal model parameters $\theta^*$ forms a three dimensional regression surface

$$
(x, y, f(\theta^*; x, y))
$$

## 2. code completion

- complete `util.py` code
- run all the cells in the jupyter notebook
- make `git commit` for the `util.py` file in such a way that your working progress is effectively presented

## 3. submission

1. jupyter notebook source files [.ipynb] - complete the jupyter notebook with all the results
   - `07.ipynb`
2. jupyter notebook pdf files [.pdf] - export the complete jupyter notebook to HTML and then PDF
   - `07.pdf`
3. GitHub history pdf file [.pdf] - export the GitHub history of the utility python file to a pdf file
   - `07_history.pdf`
4. python files [.py] - complete the utility python file
   - `util.py`
